\section{Introduction}

As machine learning systems become more widely deployed, including in settings such as resume screening, hiring, lending, and recommendation systems, people have begun to respond to them strategically. Often, this takes the form of ``gaming the system'' or using an algorithmic system's rules and procedures to manipulate it and achieve desired outcomes. Examples include Uber drivers coordinating the times they log on and off the app to impact its surge pricing algorithm \cite{mohlmann2017hands}, and Twitter \cite{burrell2019users} and Facebook \cite{eslami2016first} users' decisions regarding how to interact with content given the platforms' curation algorithms. 

Game theoretical modeling and analysis have been used in recent years to formally analyze such strategic responses of humans to algorithms (e.g., \cite{Hardt2016strategic, Milli2019socialcost, Liu2020disparateequilibria}; see also Related Work). However, these existing works assume \emph{standard} models of decision making, where agents are fully rational when responding to algorithms; yet, humans exhibit different forms of cognitive biases in decision making \cite{kahnemann1979prospect}. Motivated by this, we explore the impacts \emph{behavioral biases} on agents' strategic responses to algorithms. 

We begin by proposing a model of strategic classification that accounts for agents' behavioral biases. Specifically, our model accounts for agents misperceiving (i.e., over- or under-weighing) the importance of different features in determining the classifier's output. Such feature importance/contribution weights may be known to agents if one assumes a full information game, or can become available to them when the firm offers explanations through an Explainable AI (XAI) method which provides information about feature importance/contribution in the algorithm (e.g. SHAP~\cite{lundberg2017shap} or LIME~\cite{ribeiro2016lime}). We use this model to identify different forms of discrepancies that can arise between behavioral and fully rational agents' responses to the classifier (Lemmas~\ref{lemma:band-optimization}-\ref{lemma:manhattan-cost-band}). We further identify conditions under which agents' behavioral biases lead them to over- or under-invest in specific features (Proposition~\ref{prop:under-invest-high-dim}). Moreover, we show that a firm's utility could increase or decrease when agents are behaviorally biased, compared to when they are fully rational (Proposition~\ref{prop:mismatch-actual-b}). While the former may be intuitively expected (behaviorally biased agents are less adept at gaming algorithms), the latter is more surprising; we  provide an intuitive explanation for this through a numerical example (Example~\ref{ex:firm-benefit-hurt}), highlighting the impact of agents' qualification states in determining the ultimate impact of agents' behavioral biases on the firm. 

Finally, by conducting a user study, we show that this type of behavioral bias is present when individuals interact with an AI decision assistant. Our study shows that individuals' responses can be interpreted as users underestimating the importance of the most crucial feature, while overestimating the importance of the least important one. We also find that increasing the complexity of the model, either by adding more features or having unbalanced feature weights, amplifies this bias. Additionally, we observe other forms of cognitive biases (not captured by probability weighting biases), such as some individuals disproportionately investing in a feature with a lower starting point when feature weights are similar.

Together, our theoretical findings and user studies highlight the necessity of accounting for not just strategic responses but also cognitive biases when designing AI systems with human in the loop. 

\textbf{Summary of contributions.} Our main contributions are summarized below.

\textbullet\, We propose a new model of strategic classification which accounts for agents' cognitive biases (in the form of probability weighing biases) when perceiving the importance of a classifier's features.

\textbullet\, We analyze how and why these biases can lead to over- or under-investment in certain features compared to fully rational agents. We further show that behaviorally biased agents can increase or decrease firm utility, establishing the potential mixed effect of users' behavioral biases on firms implementing algorithmic decision systems.

\textbullet\, Through a user study, we confirm that cognitive biases influence human's understanding of, and responses to, AI systems, especially when participants are given (explanations of) models with unbalanced feature weights and a higher number of features, with the exhibited biases aligning with our model choices.

\subsection{Related Work} 
Our work is closely related to the literature on analyzing agents' responses to machine learning algorithms, when agents have full \cite{Hardt2016strategic, Perdomo2020performative, Milli2019socialcost, Hu2019disparate, Liu2020disparateequilibria, kleinberg202induce, alhanouti2024could, pmlr-v162-zhang22l, bechavod2021gaming} or partial information \cite{haghtalab2023calibratedstackelberggameslearning,cohen2024bayesian, bechavod2022information, harris2022bayesian, Ghalme2021StrategicCI, chen2023persuading, de2022non} about the algorithm. While our base model of agents' strategic responses to (threshold) classifiers has similarities to those in some of these works (e.g., \cite{Hu2019disparate, Liu2020disparateequilibria}), we differ in our modeling of agent's \emph{behavioral} responses as opposed to fully \emph{rational} (non-behavioral) best responses considered in these works. We also note that our work shares a common feature with those where agents have partial information about algorithms~\cite{Ghalme2021StrategicCI, chen2023persuading, de2022non}: in both ours and this line of work, agents respond to a classifier different from the one actually implemented. That said, we take a distinct approach from these existing works: rather than assuming agents lack access to the true classifier, we show that even when the algorithm is fully and truthfully explained, agents can still exhibit suboptimal responses due to their inherent biases. This highlights a fundamental limitation in agents' optimally responding to information on algorithmic systems, even when available to them. 


The necessity of accounting for human biases when designing and developing algorithmic models has been considered in recent work \cite{Morewedge2023bias, zhu2024capturingcomplexityhumanstrategic, liu2024largelanguagemodelsassume,heidari2021perceptions}. Among these, \cite{heidari2021perceptions} uses probability weighting functions to model human perceptions of allocation policies. We also consider (Prelec) weighting functions at times, but only to highlight special cases of our results. We also differ from all these works in our focus on the \emph{strategic classification} problem.

Broadly, our research is also related to the area of explainable machine learning. While explanations can be helpful in increasing accountability, there is debate about the efficacy of existing explainability methods in providing correct and sufficient details in a way that helps users understand and act around these systems \cite{doshivelez2019accountabilityailawrole, kumar2020shapproblem, lakkaraju2020fool, adebayo2018sanity}. To complement these discussions, our work provides a formal model of how agents' behavioral biases may shape their responses to explanations (of feature importance) provided to them. We further confirm the presence of behavioral biases through a user study. Previous works have utilized user studies to assess interpretable models based on factors such as time spent, number of words, and accuracy \cite{lakkaraju2016decisionsets}, to establish the core principles of interpretability goals \cite{hong2020humanfactors}, and to assess the impact of model interpretability on predicting model outputs \cite{poursabzi2021manipulating}. In contrast, we assess how behavioral biases can result in human subjects' sub-optimal responses to interpretable models. 

We review additional related work in Appendix~\ref{sec:app-lit-review}.  

\section{Additional related work}\label{sec:app-lit-review}
\textbf{Iterpretable Machine Learning and Explainable AI:} The interpretability of machine learning models and explainable AI is receiving more attention as it becomes more necessary for firms in various fields of work to explain an AI decision-making assistant or understand the algorithm's process for that specific decision \cite{ALI2023101805, peeking2018adadi}. Many studies have focused on providing guidelines and new objectives for the algorithm to ensure interpretability, \cite{freitas2014position} discusses the interpretability issues for five specific classification models and more generic issues of interpretability. \cite{lakkaraju2016decisionsets} provides a multi-objective optimization problem and uses model interpretability as a goal of the learning algorithm, \cite{lundberg2017shap} and \cite{ribeiro2016lime} provide methods for posthoc explanations. \cite{adebayo2018sanity} provides a method to evaluate an explanation method for image data. Many other works have also conducted user studies for finding or evaluating the guidelines using measures such as satisfaction, understanding, trust, etc. \cite{poursabzi2021manipulating, kulesza2015principles, sixt2022do}. 

\textbf{Strategic Classification:} Explanations enable users to potentially respond \cite{Camacho2011manipulation} to an algorithm to improve, meaning that they change their features to change their actual qualification or cheat, meaning that they manipulate their features to game the algorithm. This topic is extensively discussed in works such as \cite{Perdomo2020performative, Hardt2016strategic, Liu2020disparateequilibria, bechavod2021gaming, Hu2019disparate}. However, these works assume complete information on the model parameters, which is not necessarily a correct assumption. \cite{cohen2024bayesian} explores the partial information released by the firm and discusses the firm's optimization problem and agents' best response. \cite{haghtalab2023calibratedstackelberggameslearning} introduced the calibrated Stackelberg games where the agent does not have direct access to the firm's action. This can also be implemented in our framework where the firm uses $\vtheta$ but announces $\vtheta'$, and agents respond to $\vw(\vtheta')$. Another line of work called actionable recourse suggests giving actionable responses to users alongside the model explanation could be beneficial and help the users have a better outcome \cite{karimi2022recoursesurvey}. \cite{ustun2019recourse} provides an integer programming toolkit and introduces actionable recourse in linear regression. \cite{karimi2021algorithmicrecourse} introduces algorithmic recourse that allows people to act rather than understand. \cite{harris2022bayesian} combines the algorithmic recourse with partial information and has a firm that provides actionable recourse and steers agents. They show that agents and the firm are never worse off in expectation in this setting. 